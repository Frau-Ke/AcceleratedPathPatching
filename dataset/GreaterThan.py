import torch as t
import pandas as pd
import numpy as np
import random
from transformers import AutoTokenizer
from utils.dataset_loader import start_of_prompt, end_of_prompt
from torch.utils.data import TensorDataset
from torch.nn.utils.rnn import pad_sequence

def get_year_idx(prompts, years, tokenizer, prepend_bos:bool=False):
    
    year_idx_dict = {}
    year_idx_dict["YEAR"] = []
    year_idx_dict["TARGET_YEAR"] = []
    for prompt, year in zip(prompts, years):
        text_split = prompt.split(" ")
        toks = tokenizer.tokenize(" ".join(text_split))
        year_prompt = tokenizer.tokenize(year)[0]

        year_idx = toks.index(year_prompt)
        target_year_idx = len(toks) - 2
        year_idx_dict["YEAR"].append(year_idx)
        year_idx_dict["TARGET_YEAR"].append(target_year_idx)
    
    return [
        int(prepend_bos) + t.tensor(year_idx_dict[idx_type])
        for idx_type in ["YEAR", "TARGET_YEAR"]
    ]
    
def get_idx_dict(prompts, years, tokenizer, prepend_bos:bool=False):
    year_idx, target_year_idx = get_year_idx(prompts, years, tokenizer, prepend_bos)
    
    return {
        "YEAR": year_idx,
        "TARGET_YEAR": target_year_idx
    }
    

class GreaterThan():
    def __init__(
        self, 
        model_name:str,
        N:int,  
        device:str, 
        seed:int, 
        tokenizer, 
        file_path: str="/home/eickhoff/esx670/OverlapMetric/dataset/greater_than_data.csv", 
        prepend_bos:bool=False
        ) -> None:
        """Create samples for the "greater than" task. Samples of this task are of the form:
            "The war lasted from 17XX to 17YY"
        The model has to predict YY. It is correct, if YY > XX.
        Args:
            N (int): Number of samples
            device (str): device
            seed (int): seed
            tokenizer (_type_): tokenizer
            file_path (str, optional): path to .csv table. Defaults to "dataset/greater_than_data.csv".
        """
        print(f"Load GreaterThan with {model_name}")

        if tokenizer is None:
            print("no tokenizer selected, choose gpt2 tokenizer per default")
            self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        else:
            self.tokenizer = tokenizer


        self.device = device
        self.N = N
        self.seed = seed
        random.seed(self.seed)
        np.random.seed(self.seed)
        t.manual_seed(self.seed)
        
        generator = t.Generator()
        generator.manual_seed(self.seed)

        # read dataexamples from csv file
        csv_data = pd.read_csv(file_path)
        indices = t.randint(0, len(csv_data), (N,), generator=generator)    
        pd_data = csv_data.iloc[indices]
        
        years = pd_data["label"].values.tolist()
        self.years = [str(number) for number in years]
        
        # convert the strings "2" to "9" to strings of the form "02" to "09"
        # important because using different tokens
        
        # also add a random, but correct year at the end of the prompt (correct, iff rand_year = (year, 99])
        # convert into strings, and fix length if needed
        random_year = []
        for i in range(len(self.years)):
            passed_checks = False
            counter=0
            
            while not passed_checks:
                case1, case2 = False, False
                rand_year = t.randint(years[i] + 1, 99, size=(1,)).item()
                rand_year = str(rand_year)
                if len(self.years[i]) < 2:
                    self.years[i] = "0"+ self.years[i]        
                if len(rand_year) < 2:
                    rand_year = "0"+rand_year


                prompt_to_list = pd_data["clean"].values.tolist()[i].split(" ")                
                if len(tokenizer(" " + prompt_to_list[-1]+ rand_year).input_ids) == 2:
                    # case that the whole number is generated by only one token
                    case1=True
                
                year_toks = tokenizer(" "+ prompt_to_list[-1]+ rand_year).input_ids
                if len(tokenizer.decode(year_toks[0])) == 3: # space + 2 numbers
                    case2 = True
                passed_checks = case1 and case2
                if counter >= 5:
                    continue
                counter += 1
                    
            random_year.append(rand_year)

                            
        
        
        
        ##### Clean inputs and tokens
        self.clean_input = [prompt + rand_year for prompt, rand_year in zip(pd_data["clean"].values.tolist(), random_year)]
        texts_clean = [
                        (self.tokenizer.bos_token if prepend_bos else "") + prompt
                                    for prompt in self.clean_input
                                            
        ]
        self.clean_tokens = t.tensor(self.tokenizer(texts_clean, padding=True).input_ids).long().to(self.device)


        #### Corrupted inputs and tokens
        self.corrupted_input = [prompt + rand_year for prompt, rand_year in zip(pd_data["corrupted"].values.tolist(), random_year)]
        texts_corrupted = [
                (self.tokenizer.bos_token if prepend_bos else "") + prompt 
                for prompt in self.corrupted_input
            ]   
        self.corrupted_tokens = t.tensor(self.tokenizer(texts_corrupted, padding=True).input_ids).long().to(self.device)
            
                
        ##### Get the correct and wrong answer tokens
        mask_token = tokenizer.eos_token_id
        # all years yy, which are bigger than the label year are considered a correct answer
        # all years yy which are smaller than the label year are considered a wrong answer
        min_year = min(years) + 1     
        max_year = max(years)   

        #self.correct_answer = t.full((N, 99 - min_year,), fill_value = mask_token)
        #self.wrong_answer = t.full((N, max_year - 1), fill_value = mask_token)
        
        self.answer_tokens = {}
        #self.answer_tokens["correct"] = []
        #self.answer_tokens["wrong"] = []
        self.correct_answers = []
        self.wrong_answers = []
        
        for i in range(N):
            ca = [str(num).zfill(2) for num in list(range(years[i] + 1, 100))]
            ca = t.tensor(self.tokenizer(ca).input_ids).squeeze(1)
            wa = [str(num).zfill(2) for num in list(range(1, years[i]))]
            wa = t.tensor(self.tokenizer(wa).input_ids).squeeze(1)
            #self.answer_tokens["correct"].append(ca.to(device))
            #self.answer_tokens["wrong"].append(wa.to(device))
            self.correct_answers.append(ca.to(device))
            self.wrong_answers.append(wa.to(device))
            
        # max_len and groups
        self.max_len = self.clean_tokens.size(1)
        
        self.groups = [np.array(range(len(self.clean_tokens)))]
        
        #print("groups", self.groups)
        
        #### attention mask and target idx<
        self.attention_mask = self.tokenizer(texts_clean, padding=True, return_tensors="pt").attention_mask   
        
        self.token_position = get_idx_dict(self.clean_input, self.years, tokenizer, prepend_bos)
        self.target_idx = t.stack((t.arange(self.clean_tokens.size(0))
                                        , self.token_position["TARGET_YEAR"]), dim=1)
        
        self.start = t.zeros(self.N)
        self.end = t.full((self.N,), fill_value=self.max_len)
        
        for idx in range(self.N):
            #print(self.clean_input[idx])
            #print(self.clean_tokens[self.target_idx[idx, 0]])
            #print(self.answer_tokens["correct"][idx])

            if self.clean_tokens[idx, self.target_idx[idx, 1] + 1] not in self.correct_answers[idx]:
            
                print("prompt", self.clean_input[idx])
                print("toks", self.clean_tokens[idx])
                print("correct answer tokens", self.correct_answers[idx])
                print("correct answer txt", tokenizer.batch_decode(self.wrong_answers[idx]))
                print("target idx", self.target_idx[idx, 1] + 1)
                print("token at target position", self.clean_tokens[idx, self.target_idx[idx, 1] + 1])
                print("target token", tokenizer.decode(self.clean_tokens[idx, self.target_idx[idx, 1] + 1]))
                #raise Exception("Target token does not align with the position of the correct answer in at least one of the senteces.")

        self.correct_answers = pad_sequence(self.correct_answers, batch_first=True, padding_value=-1)
        self.wrong_answers = pad_sequence(self.wrong_answers, batch_first=True, padding_value=-1)

        self.dataset = TensorDataset(
            self.clean_tokens,        # [N, seq_len], 
            self.corrupted_tokens,    # [N, seq_len]
            self.attention_mask,      # [N, seq_len]
            self.correct_answers,     # [N, max_year]
            self.wrong_answers,       # [N, 100 - min_year]            
            self.target_idx,          # [N, 2]
        )
        
    
    def __len__(self):
        return self.N
    
    def __getitem__(self, idx): 
        return {
            "clean_tokens": self.clean_tokens[idx],
            "corrupted_tokens": self.corrupted_tokens[idx],
            "mask": self.attention_mask[idx],
            "correct_answers": self.correct_answers[idx],
            "wrong_answers": self.wrong_answers[idx],
            "target_idx": self.target_idx[idx],
        }
           
    def pretty_print(self):
        for i in range(self.N):
            print(self.clean_input[i])
            year = self.clean_input[i].split(" ")[6]


    def get_valid_dates(self, tokenizer):
        valid_years = {}
        year_tokens_to_year = {}
        years_to_year_token = {}
        century_tokens_to_century = {}
        for century in range(11, 18):
            valid_years[century] = []
            century_tokens_to_century[tokenizer(" " + str(century)).input_ids[0]] = century
            
            for year in range(century*100 + 2, century*100 + 99):
                year_token = tokenizer(str(year)).input_ids
                year_decode = tokenizer.batch_decode(year_token)
                year_suffix = int(year_decode[-1])
                
                if len(year_token) == 2 and len(year_decode[0]) == 2 and len(year_decode[1]) == 2:
                    valid_years[century].append(year_suffix)
                    if year_tokens_to_year.get(year_token[-1]) is None:
                        year_tokens_to_year[year_token[-1]] = year_suffix
                    if years_to_year_token.get(year_suffix) is None:
                        years_to_year_token[year_suffix] = year_token[-1]
                    
        return valid_years, century_tokens_to_century, year_tokens_to_year, years_to_year_token
    

def get_year_idx_qwen(tokens, years, tokenizer,  prepend_bos:bool=False):
        
    year_idx_dict = {}
    year_idx_dict["START"] = []
    year_idx_dict["END"] = []
    year_idx_dict["YEAR"] = []
    year_idx_dict["TARGET_YEAR"] = []
    for toks, year in zip(tokens, years):
        year_tok = t.tensor(tokenizer(year).input_ids[0])
        
        start_idx = start_of_prompt(toks, tokenizer=tokenizer, start_text="<|im_start|>user\n")
        end_idx = end_of_prompt(toks, tokenizer=tokenizer, end_text="<|im_end|>\n")
   
        year_idx = t.nonzero(toks == year_tok)[0].item()
        
        year_idx_dict["START"].append(start_idx)
        year_idx_dict["END"].append(end_idx)
        year_idx_dict["YEAR"].append(year_idx)
        year_idx_dict["TARGET_YEAR"].append(end_idx - 3) 
    
    return [
        int(prepend_bos) + t.tensor(year_idx_dict[idx_type])
        for idx_type in ["START", "END", "YEAR", "TARGET_YEAR"]
    ]
    
def get_idx_dict_qwen(prompts, years, tokenizer, prepend_bos:bool=False):
    start_idx, end_idx, year_idx, target_year_idx = get_year_idx_qwen(prompts, years, tokenizer, prepend_bos)
    
    return {
        "START":start_idx,
        "END":end_idx,
        "YEAR": year_idx,
        "TARGET_YEAR": target_year_idx
    }
    



class GreaterThanQwen():
    def __init__(
        self, 
        model_name:str,
        N:int,  
        device:str, 
        seed:int, 
        tokenizer, 
        file_path: str="/mnt/lustre/home/eickhoff/esx670/OverlapMetric/dataset/greater_than_data.csv", 
        prepend_bos:bool=False
        ) -> None:
        
        """Create samples for the "greater than" task. Samples of this task are of the form:
            "The war lasted from 17XX to 17YY"
        The model has to predict YY. It is correct, if YY > XX.

        --- Difference for Qwen models ----
        Tokenizer encods single digites as tokens. GPT2 encodes double digits as tokens:
        "The war lastet from 17(X_1)(X_2) to 17(Y_1)(Y2)"
        The task is to predict a number that is Greater Than the give year.
        
        Metric needs to be adapted accordingly
        If Y1 == X1:
            correct answer: Y1 (Y2 | Y2 > X2) -> logit(Y1) * logit(Y2 | Y2 > X2)
            wrong answer: Y1 (Y2 | Y2 <= X2) -> logit(Y1) * logit(Y2 | Y2 > X2)
        
        else:
            correct answer Y1 > X1, Y2 egal -> logit(Y1 | Y1 > X1) * logit(Y2 | for all Y2)
            wrong answer Y1 < X1, Y2 egal -Y logit(Y1 | Y1 <= X1) * logit(Y2 | for all Y2)
        

        Args:
            N (int): Number of samples
            device (str): device
            seed (int): seed
            tokenizer (_type_): tokenizer
            file_path (str, optional): path to .csv table. Defaults to "dataset/greater_than_data.csv".
        """
        
        print(f"loading GreaterThan dataset for {model_name}")

        if tokenizer is None:
            print("no tokenizer selected, choose Qwen tokenizer per default")
            self.tokenizer = AutoTokenizer.from_pretrained("Qwen")
        else:
            self.tokenizer = tokenizer

        self.device = device
        self.N = N
        self.seed = seed
        random.seed(self.seed)
        np.random.seed(self.seed)
        t.manual_seed(self.seed)
        
        generator = t.Generator()
        generator.manual_seed(self.seed)

        # ------ read dataexamples from csv file -------
        csv_data = pd.read_csv(file_path)
        indices = t.randint(0, len(csv_data), (N,), generator=generator)    
        pd_data = csv_data.iloc[indices]
        
        years = pd_data["label"].values.tolist()
        self.years = [str(number) for number in years]

        
        # ----- Adapt prompts ------
        # convert the strings "2" to "9" to strings of the form "02" to "09"
        # important because using different tokens
        
        # also add a random, but correct year at the end of the prompt (correct, iff rand_year = (year, 99])
        # convert into strings, and fix length if needed
        random_year = []
        for i in range(len(self.years)):
            rand_year = t.randint(years[i], 99, size=(1,)).item()
            rand_year = str(rand_year)
            if len(self.years[i]) < 2:
                self.years[i] = "0"+ self.years[i]        
            if len(rand_year) < 2:
                random_year.append("0"+rand_year)
            else:
                random_year.append(rand_year)
        
        
        # ------Clean inputs and tokens ------
        self.clean_input = [prompt + rand_year for prompt, rand_year in zip(pd_data["clean"].values.tolist(), random_year)]
        texts = [
            (self.tokenizer.bos_token if prepend_bos else "") + prompt
                        for prompt in self.clean_input                                
        ]
        
        if False:
        
            texts_clean = [
                tokenizer.apply_chat_template(
                    [{"role": "user", "content": prompt}],
                    tokenize=False,
                    add_generation_prompt=True
                ) for prompt in texts
            ]
            texts = texts_clean
            
        self.clean_tokens = t.tensor(self.tokenizer(texts, padding=True).input_ids).long().to(self.device)

        # ------ Corrupted inputs and tokens ------
        self.corrupted_input = [prompt + rand_year for prompt, rand_year in zip(pd_data["corrupted"].values.tolist(), random_year)]
        texts = [
            (self.tokenizer.bos_token if prepend_bos else "") + prompt
                        for prompt in self.corrupted_input                                
        ]

        if False:
            texts_corrupted = [
                tokenizer.apply_chat_template(
                    [{"role": "user", "content": prompt}],
                    tokenize=False,
                    add_generation_prompt=True
                ) for prompt in texts
            ]
            texts = texts_corrupted
        
        self.corrupted_tokens = t.tensor(self.tokenizer(texts, padding=True).input_ids).long().to(self.device)
                
        # ---- Get the correct and wrong answer tokens ----
        mask_token = tokenizer.eos_token_id
        
        # all years for which Y1 > X1 or Y1 == X1 and Y2 > X2 are considered right answers
        # all years yy which are smaller than the label year are considered a wrong answer
        
        self.answer_tokens = {}
        self.answer_tokens["correct"] = []
        self.answer_tokens["wrong"] = []
        
        
        for i in range(N):
            X1, X2 = self.years[i]
            X1 = int(X1)
            X2 = int(X2)
            
            ca = [[str(Y1), str(Y2)] for Y1 in range(10) for Y2 in range(10) if Y1 > X1 or (Y1 == X1 and Y2 > X2)]
            ca = t.tensor(self.tokenizer(ca).input_ids).squeeze(1)
            wa = [[str(Y1), str(Y2)] for Y1 in range(10) for Y2 in range(10) if Y1 < X1 or (Y1 == X1 and Y2 <= X2)]
            wa = t.tensor(self.tokenizer(wa).input_ids).squeeze(1)
            
            
            self.answer_tokens["correct"].append(ca.to(device))
            self.answer_tokens["wrong"].append(wa.to(device))
        
        # max_len and groups
        self.max_len =self.clean_tokens.size(1)
        self.groups = [np.array(range(len(self.clean_tokens)))]
        
        #### attention mask and target idx
        self.attention_mask = self.tokenizer(texts, padding=True, return_tensors="pt").attention_mask
         
        
        self.word_idx_dict = get_idx_dict_qwen(self.clean_tokens, self.years, tokenizer, prepend_bos)
        #self.token_position = get_idx_dict_qwen(self.clean_tokens, self.years, tokenizer, prepend_bos)
        self.target_idx = t.stack((t.arange(self.clean_tokens.size(0))
                                        , self.word_idx_dict["TARGET_YEAR"]), dim=1)
        
        # ----- mainly for plotting activations patterns. Assumse one sample -----
        self.start = self.word_idx_dict["START"]
        self.end = self.word_idx_dict["END"]
        
        for idx in range(self.N):
            
            #print(self.clean_input[idx])
            #print(self.clean_tokens[self.target_idx[idx, 0]])
            #print(self.answer_tokens["correct"][idx])

            if self.clean_tokens[idx, self.target_idx[idx, 1] + 1] not in self.answer_tokens["correct"][idx]:
            
                print("prompt", self.clean_input[idx])
                print("toks", self.clean_tokens[idx])
                print("correct answer tokens", self.answer_tokens["correct"][idx])
                print("correct answer txt", tokenizer.batch_decode(self.answer_tokens["correct"][idx]))
                print("target idx", self.target_idx[idx, 1] + 1)
                print("token at target position", self.clean_tokens[idx, self.target_idx[idx, 1] + 1])
                print("target token", tokenizer.decode(self.clean_tokens[idx, self.target_idx[idx, 1] + 1]))
                #raise Exception("Target token does not align with the position of the correct answer in at least one of the senteces.")
            #else:
            #    print("Greater Than passed all checks")
        #print(self.answer_tokens)
        self.correct_answers = self.answer_tokens["correct"]
        self.wrong_answers = self.answer_tokens["wrong"]
        
        self.correct_answers = pad_sequence(self.correct_answers, batch_first=True, padding_value=-1)
        self.wrong_answers = pad_sequence(self.wrong_answers, batch_first=True, padding_value=-1)
        self.dataset = TensorDataset(
            self.clean_tokens,        # [N, seq_len], 
            self.corrupted_tokens,    # [N, seq_len]
            self.attention_mask,      # [N, seq_len]
            self.correct_answers,     # [N, max_year]
            self.wrong_answers,       # [N, 100 - min_year]            
            self.target_idx,          # [N, 2]
        )        
    
    def __len__(self):
        return self.N
    
    def __getitem__(self, idx): 
        return {
            "clean_tokens": self.clean_tokens[idx],
            "corrupted_tokens": self.corrupted_tokens[idx],
            "mask": self.attention_mask[idx],
            "correct_answers": self.correct_answers[idx],
            "wrong_answers": self.wrong_answers[idx],
            "target_idx": self.target_idx[idx],
        }
        
    def pretty_print(self):
        for i in range(self.N):
            print(self.clean_input[i])
            year = self.clean_input[i].split(" ")[6]
        #print("century tokens to century", self.CENTURY_TOKENS_TO_CENTURY)
        #print("year tokens to year", self.YEAR_TOKENS_TO_YEAR)
        

    def get_valid_dates(self, tokenizer):
        valid_years = {}
        year_tokens_to_year = {}
        years_to_year_token = {}
        century_tokens_to_century = {}
        for century in range(11, 18):
            valid_years[century] = []
            century_tokens_to_century[tokenizer(" " + str(century)).input_ids[0]] = century
            
            for year in range(century*100 + 2, century*100 + 99):
                year_token = tokenizer(str(year)).input_ids
                year_decode = tokenizer.batch_decode(year_token)
                year_suffix = int(year_decode[-1])
                
                if len(year_token) == 2 and len(year_decode[0]) == 2 and len(year_decode[1]) == 2:
                    valid_years[century].append(year_suffix)
                    if year_tokens_to_year.get(year_token[-1]) is None:
                        year_tokens_to_year[year_token[-1]] = year_suffix
                    if years_to_year_token.get(year_suffix) is None:
                        years_to_year_token[year_suffix] = year_token[-1]
                    
        return valid_years, century_tokens_to_century, year_tokens_to_year, years_to_year_token

class Dataset():
    def __init__(self, input_dataset, N:int=500, tokenizer=None, device:str="cuda"):

        self.N = N
        self.toks =  t.tensor(tokenizer(input_dataset).input_ids).to(device)
        self.max_len = max([len(prompt) for prompt in self.toks])
        # since there are no different templates, all inputs belong to one group
        self.groups = [np.array(range(len(self.toks)))]